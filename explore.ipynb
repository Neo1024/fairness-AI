{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrimination(data, target, sens, expl, max_corr=.1):\n",
    "    # target\n",
    "    # sens: sensitive attribute\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        P_star = (np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i)) + \n",
    "                  np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i)))/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i)), \n",
    "             P_star)\n",
    "#         print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "#              (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "#              np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "#              np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "    print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "    D_illegal = D_all - D_expl\n",
    "    print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return (D_all, D_expl, D_illegal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local massaging\n",
    "def local_massaging(data, target, sens, expl, t_feature, max_corr=.1):\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    # t_feature: training feature for the ranker\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "    \n",
    "    data['cluster_label'] = data_expl\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    e_subgroups = []\n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        fav_prob = np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i))\n",
    "        non_fav_prob = np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i))\n",
    "        P_star = (fav_prob + non_fav_prob)/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "            print('One group is absent for this explainable value')\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             fav_prob, non_fav_prob, P_star)\n",
    "        print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "             (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "        # get sub group of current explainable value\n",
    "        sub_grp = data[data_expl == e_i].copy()\n",
    "        sub_grp.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # calculate the number of samples that need to be flipped\n",
    "        delta_priv = int(round(abs(np.sum((sub_grp[sens]==2) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==2) * P_star))))\n",
    "        delta_prot = int(round(abs(np.sum((sub_grp[sens]==1) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==1) * P_star))))\n",
    "        if P_star == 0:\n",
    "            delta_priv = 0\n",
    "            delta_prot = 0\n",
    "        print('Required number of flipping samples for favourable and non-favourable group: ', \n",
    "              delta_priv, delta_prot)\n",
    "        \n",
    "        # get scores and rank\n",
    "        score, ranking =  get_ranking_score(sub_grp, t_feature)\n",
    "        sub_grp['score'] = score\n",
    "        sub_grp.sort_values('score', ascending=False, inplace=True)\n",
    "        \n",
    "        # flip the label for last samples in each group\n",
    "        sub_grp_priv_pos = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_pos = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_priv_neg = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_neg = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        if P_star != 0:\n",
    "            if fav_prob > non_fav_prob:\n",
    "                sub_grp_priv_pos.iloc[-delta_priv:, 0] = 0\n",
    "                sub_grp_prot_neg.iloc[:delta_prot, 0] = 1\n",
    "            else:\n",
    "                sub_grp_priv_neg.iloc[:delta_priv, 0] = 1\n",
    "                sub_grp_prot_pos.iloc[-delta_prot:, 0] = 0\n",
    "            \n",
    "#         print('Favourable group prob, Non-favourable group prob: ', \n",
    "#              np.sum(sub_grp_priv[target]==1)/len(sub_grp_priv), \n",
    "#              np.sum(sub_grp_prot[target]==1)/len(sub_grp_prot))\n",
    "        print('********************************')\n",
    "            \n",
    "        # append to arrays\n",
    "        e_subgroups.append(sub_grp_priv_pos)\n",
    "        e_subgroups.append(sub_grp_prot_pos)\n",
    "        e_subgroups.append(sub_grp_priv_neg)\n",
    "        e_subgroups.append(sub_grp_prot_neg)\n",
    "    \n",
    "    revised_df = pd.concat(e_subgroups, ignore_index=False)\n",
    "    revised_df.reset_index(inplace=True, drop=True)\n",
    "    print(revised_df.info())\n",
    "        \n",
    "#     print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "#     D_illegal = D_all - D_expl\n",
    "#     print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return revised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local massaging\n",
    "def local_preferential_samping(data, target, sens, expl, t_feature, max_corr=.1):\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    # t_feature: training feature for the ranker\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "    \n",
    "    data['cluster_label'] = data_expl\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    e_subgroups = []\n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        fav_prob = np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i))\n",
    "        non_fav_prob = np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i))\n",
    "        P_star = (fav_prob + non_fav_prob)/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "            print('One group is absent for this explainable value')\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             fav_prob, non_fav_prob, P_star)\n",
    "        print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "             (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "        # get sub group of current explainable value\n",
    "        sub_grp = data[data_expl == e_i].copy()\n",
    "        sub_grp.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # calculate the number of samples that need to be flipped\n",
    "        delta_priv = int(round(abs(np.sum((sub_grp[sens]==2) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==2) * P_star))))\n",
    "        delta_prot = int(round(abs(np.sum((sub_grp[sens]==1) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==1) * P_star))))\n",
    "        if P_star == 0:\n",
    "            delta_priv = 0\n",
    "            delta_prot = 0\n",
    "        print('Required number of flipping samples for favourable and non-favourable group: ', \n",
    "              delta_priv, delta_prot)\n",
    "        \n",
    "        # get scores and rank\n",
    "        score, ranking =  get_ranking_score(sub_grp, t_feature)\n",
    "        sub_grp['score'] = score\n",
    "        sub_grp.sort_values('score', ascending=False, inplace=True)\n",
    "        \n",
    "        # flip the label for last samples in each group\n",
    "        sub_grp_priv_pos = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_pos = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_priv_neg = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_neg = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        if P_star != 0:\n",
    "            if fav_prob > non_fav_prob:\n",
    "                sub_grp_priv_pos.drop(range(len(sub_grp_priv_pos)-int(0.5*delta_priv), len(sub_grp_priv_pos)))\n",
    "                sub_grp_priv_neg.append(sub_grp_priv_neg.iloc[:int(0.5*delta_priv)])\n",
    "                sub_grp_prot_pos.append(sub_grp_prot_pos.iloc[-int(0.5*delta_prot):])\n",
    "                sub_grp_prot_neg.drop(range(int(0.5*delta_prot)))\n",
    "            else:\n",
    "                sub_grp_prot_pos.drop(range(len(sub_grp_prot_pos)-int(0.5*delta_prot), len(sub_grp_prot_pos)))\n",
    "                sub_grp_prot_neg.append(sub_grp_prot_neg.iloc[:int(0.5*delta_prot)])\n",
    "                sub_grp_priv_pos.append(sub_grp_priv_pos.iloc[-int(0.5*delta_priv):])\n",
    "                sub_grp_priv_neg.drop(range(int(0.5*delta_priv)))\n",
    "            \n",
    "#         print('Favourable group prob, Non-favourable group prob: ', \n",
    "#              np.sum(sub_grp_priv[target]==1)/len(sub_grp_priv), \n",
    "#              np.sum(sub_grp_prot[target]==1)/len(sub_grp_prot))\n",
    "        print('********************************')\n",
    "            \n",
    "        # append to arrays\n",
    "        e_subgroups.append(sub_grp_priv_pos)\n",
    "        e_subgroups.append(sub_grp_prot_pos)\n",
    "        e_subgroups.append(sub_grp_priv_neg)\n",
    "        e_subgroups.append(sub_grp_prot_neg)\n",
    "    \n",
    "    revised_df = pd.concat(e_subgroups, ignore_index=False)\n",
    "    revised_df.reset_index(inplace=True, drop=True)\n",
    "    print(revised_df.info())\n",
    "        \n",
    "#     print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "#     D_illegal = D_all - D_expl\n",
    "#     print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return revised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic model to get ranking and score (probability of being positive) for each smaple\n",
    "def get_ranking_score(data, features):\n",
    "    # features: features used for training the ranker\n",
    "    # return: two arrays, ranking and scores\n",
    "    lr = LogisticRegression().fit(data[features], data['Creditability'])\n",
    "#     coefs = dict(zip(X.columns, np.round(list(lr.coef_[0]), 2)))\n",
    "    y_prob = lr.predict_proba(data[features])[:, 1]\n",
    "    ranking = np.argsort(y_prob)[::-1]\n",
    "#     print(y_prob)\n",
    "#     print(ranking)\n",
    "#     print(y_prob[ranking])\n",
    "    return y_prob, ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Creditability', 'Account Balance', 'Duration of Credit (month)',\n",
      "       'Payment Status of Previous Credit', 'Purpose', 'Credit Amount',\n",
      "       'Value Savings/Stocks', 'Length of current employment',\n",
      "       'Instalment per cent', 'Sex & Marital Status', 'Guarantors',\n",
      "       'Duration in Current address', 'Most valuable available asset',\n",
      "       'Age (years)', 'Concurrent Credits', 'Type of apartment',\n",
      "       'No of Credits at this Bank', 'Occupation', 'No of dependents',\n",
      "       'Telephone', 'Foreign Worker', 'gender'],\n",
      "      dtype='object')\n",
      "Total discrimination: 0.166\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9789029535864979 0.684981684981685 0.8319423192840915\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9230769230769231 0.5846153846153846 0.7538461538461538\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9635036496350365 0.732484076433121 0.8479938630340788\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.0 0.2 0.1\n",
      "Discrimination explainable by Account Balance, Duration of Credit (month), Payment Status of Previous Credit, Purpose, Credit Amount, Value Savings/Stocks, Instalment per cent, Guarantors, Duration in Current address, Most valuable available asset, Concurrent Credits, Type of apartment, No of Credits at this Bank, Occupation: -0.083\n",
      "Unexplainable discrimination: 0.249\n",
      "Total discrimination: 0.166\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9789029535864979 0.684981684981685 0.8319423192840915\n",
      "Total samples, postive favourable group samples, positive non-favourable group samples:  510 232 187\n",
      "Required number of flipping samples for favourable and non-favourable group:  35 40\n",
      "********************************\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9230769230769231 0.5846153846153846 0.7538461538461538\n",
      "Total samples, postive favourable group samples, positive non-favourable group samples:  130 60 38\n",
      "Required number of flipping samples for favourable and non-favourable group:  11 11\n",
      "********************************\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9635036496350365 0.732484076433121 0.8479938630340788\n",
      "Total samples, postive favourable group samples, positive non-favourable group samples:  294 132 115\n",
      "Required number of flipping samples for favourable and non-favourable group:  16 18\n",
      "********************************\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.0 0.2 0.1\n",
      "Total samples, postive favourable group samples, positive non-favourable group samples:  66 0 1\n",
      "Required number of flipping samples for favourable and non-favourable group:  6 0\n",
      "********************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 24 columns):\n",
      "Creditability                        1000 non-null int64\n",
      "Account Balance                      1000 non-null int64\n",
      "Duration of Credit (month)           1000 non-null int64\n",
      "Payment Status of Previous Credit    1000 non-null int64\n",
      "Purpose                              1000 non-null int64\n",
      "Credit Amount                        1000 non-null int64\n",
      "Value Savings/Stocks                 1000 non-null int64\n",
      "Length of current employment         1000 non-null int64\n",
      "Instalment per cent                  1000 non-null int64\n",
      "Sex & Marital Status                 1000 non-null int64\n",
      "Guarantors                           1000 non-null int64\n",
      "Duration in Current address          1000 non-null int64\n",
      "Most valuable available asset        1000 non-null int64\n",
      "Age (years)                          1000 non-null int64\n",
      "Concurrent Credits                   1000 non-null int64\n",
      "Type of apartment                    1000 non-null int64\n",
      "No of Credits at this Bank           1000 non-null int64\n",
      "Occupation                           1000 non-null int64\n",
      "No of dependents                     1000 non-null int64\n",
      "Telephone                            1000 non-null int64\n",
      "Foreign Worker                       1000 non-null int64\n",
      "gender                               1000 non-null int64\n",
      "cluster_label                        1000 non-null int32\n",
      "score                                1000 non-null float64\n",
      "dtypes: float64(1), int32(1), int64(22)\n",
      "memory usage: 183.7 KB\n",
      "None\n",
      "Total discrimination: 0.166\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9789029535864979 0.684981684981685 0.8319423192840915\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9230769230769231 0.5846153846153846 0.7538461538461538\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9635036496350365 0.732484076433121 0.8479938630340788\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.0 0.2 0.1\n",
      "Discrimination explainable by c, l, u, s, t, e, r, _, l, a, b, e, l: -0.083\n",
      "Unexplainable discrimination: 0.249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.16599999999999993, -0.08261960150981777, 0.2486196015098177)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df = pd.read_csv('./resampled_nation_gender.csv')\n",
    "print(credit_df.columns)\n",
    "\n",
    "male_idx = (credit_df['Sex & Marital Status']==1) | (credit_df['Sex & Marital Status']==3) | \\\n",
    "                 (credit_df['Sex & Marital Status']==4)\n",
    "female_idx = (credit_df['Sex & Marital Status']==2) | (credit_df['Sex & Marital Status']==5)\n",
    "native_idx = (credit_df['Foreign Worker']==2)\n",
    "foreign_idx = (credit_df['Foreign Worker']==1)\n",
    "\n",
    "# insert a column of gender, 1 female, 2 female\n",
    "# credit_df.insert(loc=len(credit_df.columns), column='gender', value=1)\n",
    "# credit_df.loc[male_idx, 'gender'] = 2\n",
    "\n",
    "legal = credit_df.columns[ [1, 3, 5, 6, 8, 10, 12, 14, 16 ] ]\n",
    "maybe = credit_df.columns[ [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 16, 17 ] ]\n",
    "\n",
    "# discrimination(credit_df, 'Creditability', 'gender', list(maybe))\n",
    "# discrimination(credit_df, 'Creditability', 'gender', 'Occupation')\n",
    "# get_ranking_score(credit_df, list(maybe))\n",
    "discrimination(credit_df, 'Creditability', 'Foreign Worker', list(maybe))\n",
    "# revised_df = local_massaging(credit_df, 'Creditability', 'Foreign Worker', list(maybe), list(maybe))\n",
    "revised_df = local_preferential_samping(credit_df, 'Creditability', 'Foreign Worker', list(maybe), list(maybe))\n",
    "# revised_df[0:3]\n",
    "discrimination(revised_df, 'Creditability', 'Foreign Worker', 'cluster_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the discrimination in prediction using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit_df['Credit Amount'] = np.log(credit_df['Credit Amount'])\n",
    "# X = credit_df\n",
    "# y = credit_df['Creditability']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_df['Credit Amount'] = np.log(revised_df['Credit Amount'])\n",
    "X = revised_df\n",
    "y = revised_df['Creditability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.856\n",
      "(250, 24)\n",
      "Total discrimination: 0.020\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.0 0.1111111111111111 0.05555555555555555\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  1.0 0.9833333333333333 0.9916666666666667\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  0.9473684210526315 0.7272727272727273 0.8373205741626795\n",
      "Favorable group prob, non-favorable group prob, Correct prob:  nan 0.6470588235294118 0\n",
      "Discrimination explainable by Account Balance, Duration of Credit (month), Payment Status of Previous Credit, Purpose, Credit Amount, Value Savings/Stocks, Instalment per cent, Guarantors, Duration in Current address, Most valuable available asset, Concurrent Credits, Type of apartment, No of Credits at this Bank, Occupation: 0.023\n",
      "Unexplainable discrimination: -0.002\n",
      "Female and male:  125 125\n",
      "Credible Female and male:  89 110 0.712 0.88\n",
      "Foreign and native:  141 109\n",
      "Credible Foreign and native:  111 88 0.7872340425531915 0.8073394495412844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xgli/anaconda3/envs/ds/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/xgli/anaconda3/envs/ds/lib/python3.6/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(X_train[maybe], y_train)\n",
    "# coefs = dict(zip(X.columns, np.round(list(lr.coef_[0]), 2)))\n",
    "y_pred = lr.predict(X_test[maybe])\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "data_test = X_test.copy()\n",
    "print(data_test.shape)\n",
    "data_test.reset_index(inplace=True)\n",
    "data_test['Creditability'] = y_pred\n",
    "# discrimination(data_test, 'Creditability', 'gender', list(maybe))\n",
    "discrimination(data_test, 'Creditability', 'Foreign Worker', list(maybe))\n",
    "\n",
    "c_female = np.sum(data_test['gender']==1)\n",
    "c_male = np.sum(data_test['gender']==2)\n",
    "c_credible_female = np.sum((data_test['gender']==1) & (data_test['Creditability']==1))\n",
    "c_credible_male = np.sum((data_test['gender']==2) & (data_test['Creditability']==1))\n",
    "c_foreign = np.sum(data_test['Foreign Worker']==1)\n",
    "c_native = np.sum(data_test['Foreign Worker']==2)\n",
    "c_credible_foreign = np.sum((data_test['Foreign Worker']==1) & (data_test['Creditability']==1))\n",
    "c_credible_native = np.sum((data_test['Foreign Worker']==2) & (data_test['Creditability']==1))\n",
    "\n",
    "print('Female and male: ', c_female, c_male)\n",
    "print('Credible Female and male: ', c_credible_female, c_credible_male, \n",
    "      c_credible_female/c_female, c_credible_male/c_male)\n",
    "print('Foreign and native: ', c_foreign, c_native)\n",
    "print('Credible Foreign and native: ', c_credible_foreign, c_credible_native,\n",
    "      c_credible_foreign/c_foreign, c_credible_native/c_native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'col1':[1, 2, 3]})\n",
    "# df.loc[df.index[True, False, True], 'col1'] = 10\n",
    "idx = np.array(df.index)\n",
    "idx[[True, False, True]].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
