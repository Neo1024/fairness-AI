{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrimination(data, target, sens, expl, max_corr=.1):\n",
    "    # target\n",
    "    # sens: sensitive attribute\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        P_star = (np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i)) + \n",
    "                  np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i)))/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i)), \n",
    "             P_star)\n",
    "#         print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "#              (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "#              np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "#              np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "    print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "    D_illegal = D_all - D_expl\n",
    "    print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return (D_all, D_expl, D_illegal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local massaging\n",
    "def local_massaging(data, target, sens, expl, t_feature, max_corr=.1):\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    # t_feature: training feature for the ranker\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "    \n",
    "    data['cluster_label'] = data_expl\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    e_subgroups = []\n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        fav_prob = np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i))\n",
    "        non_fav_prob = np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i))\n",
    "        P_star = (fav_prob + non_fav_prob)/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "            print('One group is absent for this explainable value')\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             fav_prob, non_fav_prob, P_star)\n",
    "        print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "             (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "        # get sub group of current explainable value\n",
    "        sub_grp = data[data_expl == e_i].copy()\n",
    "        sub_grp.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # calculate the number of samples that need to be flipped\n",
    "        delta_priv = int(round(abs(np.sum((sub_grp[sens]==2) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==2) * P_star))))\n",
    "        delta_prot = int(round(abs(np.sum((sub_grp[sens]==1) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==1) * P_star))))\n",
    "        if P_star == 0:\n",
    "            delta_priv = 0\n",
    "            delta_prot = 0\n",
    "        print('Required number of flipping samples for favourable and non-favourable group: ', \n",
    "              delta_priv, delta_prot)\n",
    "        \n",
    "        # get scores and rank\n",
    "        score, ranking =  get_ranking_score(sub_grp, t_feature)\n",
    "        sub_grp['score'] = score\n",
    "        sub_grp.sort_values('score', ascending=False, inplace=True)\n",
    "        \n",
    "        # flip the label for last samples in each group\n",
    "        sub_grp_priv_pos = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_pos = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_priv_neg = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_neg = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        if P_star != 0:\n",
    "            if fav_prob > non_fav_prob:\n",
    "                sub_grp_priv_pos.iloc[-delta_priv:, 0] = 0\n",
    "                sub_grp_prot_neg.iloc[:delta_prot, 0] = 1\n",
    "            else:\n",
    "                sub_grp_priv_neg.iloc[:delta_priv, 0] = 1\n",
    "                sub_grp_prot_pos.iloc[-delta_prot:, 0] = 0\n",
    "            \n",
    "#         print('Favourable group prob, Non-favourable group prob: ', \n",
    "#              np.sum(sub_grp_priv[target]==1)/len(sub_grp_priv), \n",
    "#              np.sum(sub_grp_prot[target]==1)/len(sub_grp_prot))\n",
    "        print('********************************')\n",
    "            \n",
    "        # append to arrays\n",
    "        e_subgroups.append(sub_grp_priv_pos)\n",
    "        e_subgroups.append(sub_grp_prot_pos)\n",
    "        e_subgroups.append(sub_grp_priv_neg)\n",
    "        e_subgroups.append(sub_grp_prot_neg)\n",
    "    \n",
    "    revised_df = pd.concat(e_subgroups, ignore_index=False)\n",
    "    revised_df.reset_index(inplace=True, drop=True)\n",
    "    print(revised_df.info())\n",
    "        \n",
    "#     print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "#     D_illegal = D_all - D_expl\n",
    "#     print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return revised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local massaging\n",
    "def local_preferential_samping(data, target, sens, expl, t_feature, max_corr=.1):\n",
    "    # expl: explanatory attribute(s), str or list\n",
    "    # t_feature: training feature for the ranker\n",
    "    group_priv = data[data[sens]==2]\n",
    "    group_prot = data[data[sens]==1]\n",
    "    n_priv = group_priv.shape[0]\n",
    "    n_prot = group_prot.shape[0]\n",
    "    \n",
    "    D_all = np.sum(group_priv[target]==1)/n_priv - np.sum(group_prot[target] == 1)/n_prot\n",
    "    print('Total discrimination: %.3f' % D_all)\n",
    "    \n",
    "    # multiple explanatory attributes\n",
    "    if isinstance(expl, list):\n",
    "        high_corr = list(data.columns[np.abs(data.corr()[sens].sort_values()) > max_corr])\n",
    "        high_corr = []\n",
    "        for e in expl: \n",
    "            if e in high_corr: \n",
    "                print(e, 'is highly correlated with', sens)\n",
    "        expl = [e for e in expl if e not in high_corr]\n",
    "        data_expl = pd.Series(KMeans(n_clusters=4).fit(data[expl]).labels_)\n",
    "    else:\n",
    "        data_expl = data[expl]\n",
    "    \n",
    "    data_expl_priv = data_expl[data[sens]==2]\n",
    "    data_expl_prot = data_expl[data[sens]==1]\n",
    "    \n",
    "    data['cluster_label'] = data_expl\n",
    "        \n",
    "    expl_values = data_expl.unique()\n",
    "    D_expl = 0 \n",
    "    e_subgroups = []\n",
    "    \n",
    "    for e_i in expl_values:\n",
    "        fav_prob = np.sum((group_priv[target]==1) & (data_expl_priv == e_i))/(np.sum(data_expl_priv == e_i))\n",
    "        non_fav_prob = np.sum((group_prot[target]==1) & (data_expl_prot == e_i))/(np.sum(data_expl_prot == e_i))\n",
    "        P_star = (fav_prob + non_fav_prob)/2\n",
    "        if np.isnan(P_star): \n",
    "            P_star = 0\n",
    "            print('One group is absent for this explainable value')\n",
    "        D_expl += (np.sum(data_expl_priv == e_i)/len(data_expl_priv) -  \n",
    "                   np.sum(data_expl_prot == e_i)/len(data_expl_prot)) * P_star\n",
    "        print('Favorable group prob, non-favorable group prob, Correct prob: ', \n",
    "             fav_prob, non_fav_prob, P_star)\n",
    "        print('Total samples, postive favourable group samples, positive non-favourable group samples: ', \n",
    "             (np.sum(data_expl_priv == e_i))+(np.sum(data_expl_prot == e_i)), \n",
    "             np.sum((group_priv[target]==1) & (data_expl_priv == e_i)), \n",
    "             np.sum((group_prot[target]==1) & (data_expl_prot == e_i)))\n",
    "        \n",
    "        # get sub group of current explainable value\n",
    "        sub_grp = data[data_expl == e_i].copy()\n",
    "        sub_grp.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # calculate the number of samples that need to be flipped\n",
    "        delta_priv = int(round(abs(np.sum((sub_grp[sens]==2) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==2) * P_star))))\n",
    "        delta_prot = int(round(abs(np.sum((sub_grp[sens]==1) & (sub_grp[target]==1)) - \n",
    "                             (np.sum(sub_grp[sens]==1) * P_star))))\n",
    "        if P_star == 0:\n",
    "            delta_priv = 0\n",
    "            delta_prot = 0\n",
    "        print('Required number of flipping samples for favourable and non-favourable group: ', \n",
    "              delta_priv, delta_prot)\n",
    "        \n",
    "        # get scores and rank\n",
    "        score, ranking =  get_ranking_score(sub_grp, t_feature)\n",
    "        sub_grp['score'] = score\n",
    "        sub_grp.sort_values('score', ascending=False, inplace=True)\n",
    "        \n",
    "        # flip the label for last samples in each group\n",
    "        sub_grp_priv_pos = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_pos = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==1)].copy().reset_index(drop=True)\n",
    "        sub_grp_priv_neg = sub_grp[(sub_grp[sens]==2) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        sub_grp_prot_neg = sub_grp[(sub_grp[sens]==1) & (sub_grp[target]==0)].copy().reset_index(drop=True)\n",
    "        if P_star != 0:\n",
    "            if fav_prob > non_fav_prob:\n",
    "                sub_grp_priv_pos.drop(range(len(sub_grp_priv_pos)-int(0.5*delta_priv), len(sub_grp_priv_pos)))\n",
    "                sub_grp_priv_neg.append(sub_grp_priv_neg.iloc[:int(0.5*delta_priv)])\n",
    "                sub_grp_prot_pos.append(sub_grp_prot_pos.iloc[-int(0.5*delta_prot):])\n",
    "                sub_grp_prot_neg.drop(range(int(0.5*delta_prot)))\n",
    "            else:\n",
    "                sub_grp_prot_pos.drop(range(len(sub_grp_prot_pos)-int(0.5*delta_prot), len(sub_grp_prot_pos)))\n",
    "                sub_grp_prot_neg.append(sub_grp_prot_neg.iloc[:int(0.5*delta_prot)])\n",
    "                sub_grp_priv_pos.append(sub_grp_priv_pos.iloc[-int(0.5*delta_priv):])\n",
    "                sub_grp_priv_neg.drop(range(int(0.5*delta_priv)))\n",
    "            \n",
    "#         print('Favourable group prob, Non-favourable group prob: ', \n",
    "#              np.sum(sub_grp_priv[target]==1)/len(sub_grp_priv), \n",
    "#              np.sum(sub_grp_prot[target]==1)/len(sub_grp_prot))\n",
    "        print('********************************')\n",
    "            \n",
    "        # append to arrays\n",
    "        e_subgroups.append(sub_grp_priv_pos)\n",
    "        e_subgroups.append(sub_grp_prot_pos)\n",
    "        e_subgroups.append(sub_grp_priv_neg)\n",
    "        e_subgroups.append(sub_grp_prot_neg)\n",
    "    \n",
    "    revised_df = pd.concat(e_subgroups, ignore_index=False)\n",
    "    revised_df.reset_index(inplace=True, drop=True)\n",
    "    print(revised_df.info())\n",
    "        \n",
    "#     print('Discrimination explainable by %s: %.3f' % (', '.join(expl), D_expl))\n",
    "    \n",
    "#     D_illegal = D_all - D_expl\n",
    "#     print('Unexplainable discrimination: %.3f' % D_illegal)\n",
    "    \n",
    "    return revised_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic model to get ranking and score (probability of being positive) for each smaple\n",
    "def get_ranking_score(data, features):\n",
    "    # features: features used for training the ranker\n",
    "    # return: two arrays, ranking and scores\n",
    "    lr = LogisticRegression().fit(data[features], data['Creditability'])\n",
    "#     coefs = dict(zip(X.columns, np.round(list(lr.coef_[0]), 2)))\n",
    "    y_prob = lr.predict_proba(data[features])[:, 1]\n",
    "    ranking = np.argsort(y_prob)[::-1]\n",
    "#     print(y_prob)\n",
    "#     print(ranking)\n",
    "#     print(y_prob[ranking])\n",
    "    return y_prob, ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Creditability', 'Account Balance', 'Duration of Credit (month)',\n",
      "       'Payment Status of Previous Credit', 'Purpose', 'Credit Amount',\n",
      "       'Value Savings/Stocks', 'Length of current employment',\n",
      "       'Instalment per cent', 'Sex & Marital Status', 'Guarantors',\n",
      "       'Duration in Current address', 'Most valuable available asset',\n",
      "       'Age (years)', 'Concurrent Credits', 'Type of apartment',\n",
      "       'No of Credits at this Bank', 'Occupation', 'No of dependents',\n",
      "       'Telephone', 'Foreign Worker', 'gender'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert gender, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2f4b45ce1229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# insert a column of gender, 1 female, 2 female\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcredit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcredit_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmale_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   3220\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3221\u001b[0m         self._data.insert(loc, column, value,\n\u001b[0;32m-> 3222\u001b[0;31m                           allow_duplicates=allow_duplicates)\n\u001b[0m\u001b[1;32m   3223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ds/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4337\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4338\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot insert {}, already exists'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert gender, already exists"
     ]
    }
   ],
   "source": [
    "credit_df = pd.read_csv('./resampled_nation_gender.csv')\n",
    "print(credit_df.columns)\n",
    "\n",
    "male_idx = (credit_df['Sex & Marital Status']==1) | (credit_df['Sex & Marital Status']==3) | \\\n",
    "                 (credit_df['Sex & Marital Status']==4)\n",
    "female_idx = (credit_df['Sex & Marital Status']==2) | (credit_df['Sex & Marital Status']==5)\n",
    "native_idx = (credit_df['Foreign Worker']==2)\n",
    "foreign_idx = (credit_df['Foreign Worker']==1)\n",
    "\n",
    "# insert a column of gender, 1 female, 2 female\n",
    "# credit_df.insert(loc=len(credit_df.columns), column='gender', value=1)\n",
    "# credit_df.loc[male_idx, 'gender'] = 2\n",
    "\n",
    "legal = credit_df.columns[ [1, 3, 5, 6, 8, 10, 12, 14, 16 ] ]\n",
    "maybe = credit_df.columns[ [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 16, 17 ] ]\n",
    "\n",
    "# discrimination(credit_df, 'Creditability', 'gender', list(maybe))\n",
    "# discrimination(credit_df, 'Creditability', 'gender', 'Occupation')\n",
    "# get_ranking_score(credit_df, list(maybe))\n",
    "# discrimination(credit_df, 'Creditability', 'Foreign Worker', list(legal))\n",
    "# discrimination(credit_df, 'Creditability', 'gender', list(legal))\n",
    "# revised_df = local_massaging(credit_df, 'Creditability', 'Foreign Worker', list(legal), list(legal))\n",
    "# revised_df = local_preferential_samping(credit_df, 'Creditability', 'Foreign Worker', list(legal), list(legal))\n",
    "# revised_df[0:3]\n",
    "# discrimination(revised_df, 'Creditability', 'Foreign Worker', 'cluster_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the discrimination in prediction using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_df['Credit Amount'] = np.log(credit_df['Credit Amount'])\n",
    "X = credit_df\n",
    "y = credit_df['Creditability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised_df['Credit Amount'] = np.log(revised_df['Credit Amount'])\n",
    "# X = revised_df\n",
    "# y = revised_df['Creditability']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression().fit(X_train[legal], y_train)\n",
    "# coefs = dict(zip(X.columns, np.round(list(lr.coef_[0]), 2)))\n",
    "y_pred = lr.predict(X_test[legal])\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "data_test = X_test.copy()\n",
    "print(data_test.shape)\n",
    "data_test.reset_index(inplace=True)\n",
    "data_test['Creditability'] = y_pred\n",
    "discrimination(data_test, 'Creditability', 'gender', list(legal))\n",
    "discrimination(data_test, 'Creditability', 'Foreign Worker', list(legal))\n",
    "\n",
    "c_female = np.sum(data_test['gender']==1)\n",
    "c_male = np.sum(data_test['gender']==2)\n",
    "c_credible_female = np.sum((data_test['gender']==1) & (data_test['Creditability']==1))\n",
    "c_credible_male = np.sum((data_test['gender']==2) & (data_test['Creditability']==1))\n",
    "c_foreign = np.sum(data_test['Foreign Worker']==1)\n",
    "c_native = np.sum(data_test['Foreign Worker']==2)\n",
    "c_credible_foreign = np.sum((data_test['Foreign Worker']==1) & (data_test['Creditability']==1))\n",
    "c_credible_native = np.sum((data_test['Foreign Worker']==2) & (data_test['Creditability']==1))\n",
    "\n",
    "print('Female and male: ', c_female, c_male)\n",
    "print('Credible Female and male: ', c_credible_female, c_credible_male, \n",
    "      c_credible_female/c_female, c_credible_male/c_male)\n",
    "print('Foreign and native: ', c_foreign, c_native)\n",
    "print('Credible Foreign and native: ', c_credible_foreign, c_credible_native,\n",
    "      c_credible_foreign/c_foreign, c_credible_native/c_native)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={'col1':[1, 2, 3]})\n",
    "# df.loc[df.index[True, False, True], 'col1'] = 10\n",
    "idx = np.array(df.index)\n",
    "idx[[True, False, True]].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
